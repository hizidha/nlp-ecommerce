# -*- coding: utf-8 -*-
"""Proyek Pertama : Membuat Model NLP dengan TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HDkAmqMwh51l_KM2F_ByJgh9LTQC30TU

*   Nama Lengkap : Adisaputra Zidha Noorizki
*   Username : hi_zidha
*   Email : hi.zidha@gmail.com
"""

import pandas as pd
import tensorflow as tf
import nltk

from tensorflow import keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IDCamp2023/Intermediate/dataset/ecommerceDataset.csv',
                 header=None, names=['catg', 'description'])
df

# Mengambil 5000 data secara rata untuk setiap category
selectedData = []
uniqueCategories = df['catg'].unique()
dataCategory = 5000 // len(uniqueCategories)

for category in uniqueCategories:
    category_data = df[df['catg'] == category].sample(dataCategory, random_state=42)
    selectedData.append(category_data)

df = pd.concat(selectedData)
df

totalData = df['catg'].value_counts()
print(totalData)

category = pd.get_dummies(df.catg)
category.columns = category.columns.str.lower()

df_new = pd.concat([df, category], axis=1)
df_new = df_new.drop(columns='catg')
df_new

description = df_new['description'].values
label = df_new[['books', 'clothing & accessories', 'electronics', 'household']].values

description_train, description_val, label_train, label_val = train_test_split(description, label, test_size=0.2)

def preprocess_text(text):
    tokenizer = nltk.RegexpTokenizer(r"\w+")
    words = tokenizer.tokenize(text)
    return ' '.join(words)

tokenizer = Tokenizer(num_words=2160, oov_token='<oov>')

# Data training
description_train = [preprocess_text(text) for text in description_train]
tokenizer.fit_on_texts(description_train)
sequences_train = tokenizer.texts_to_sequences(description_train)
padded_train = pad_sequences(sequences_train, padding='post', truncating='post', maxlen=64)

# Data validation
description_val = [preprocess_text(text) for text in description_val]
sequences_val = tokenizer.texts_to_sequences(description_val)
padded_val = pad_sequences(sequences_val, padding='post', truncating='post', maxlen=64)

model = Sequential([
    Embedding(input_dim=2160, output_dim=16, input_length=64),
    LSTM(64, dropout=0.2),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(4, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
num_epochs = 25

model.summary()

history = model.fit(padded_train, label_train, epochs=num_epochs, validation_data=(padded_val, label_val), verbose=2)

import matplotlib.pyplot as plt

# Plot loss
plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.show()

# Plot Accuracy
plt.figure(figsize=(10,5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.show()